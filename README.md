# ğŸš¦ US Accidents ELT + ML Project (Databricks | Delta Lake | scikit-learn)

This project implements an end-to-end **ELT (Extractâ€“Loadâ€“Transform)** pipeline on the **US Accidents** dataset using **Databricks**, **Delta Lake**, and **scikit-learn**.  
It was developed as part of the **MADSC102 â€“ Unlocking the Power of Big Data** course.

The pipeline covers:

- Ingestion of a large CSV dataset into Databricks
- Storage in **Delta Lake**
- Cleaning & transformation with **PySpark**
- Exploratory analytics using SQL & notebooks
- A lightweight ML model using **scikit-learn**
- A final **Visualization layer** for insights

---

## ğŸ§  Overview

The goal of this project is to demonstrate how to build a **cloud-style data engineering workflow** using Databricks Community Edition:

- **Extract** US Accidents CSV from Kaggle via **Google Drive + `wget`**
- **Load** into Databricks FileStore (**DBFS**)
- **Transform** using PySpark and persist as **Delta tables**
- **Analyze** data with SQL + notebooks
- **Model** accident severity with a scikit-learn classifier
- **Orchestrate** execution via a simple â€œjob triggerâ€ notebook
- **Visualize** aggregated outputs

Because Databricks CE / Serverless has strict limits on Spark ML model size, the ML part is implemented with **pandas + scikit-learn**, which runs reliably in this environment.

---

## ğŸ— Architecture

The high-level architecture is:

- Kaggle CSV â†’ Google Drive â†’ Databricks DBFS  
- PySpark ETL â†’ Delta Lake  
- EDA Notebook + ML Notebook â†’ Visualization Layer

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚      Kaggle Dataset       â”‚
                         â”‚   (US Accidents CSV)      â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   Google Drive Storage    â”‚
                         â”‚  (Public Download Link)   â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚ wget
                                       â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚       Databricks FileStore (DBFS) â”‚
                      â”‚   /FileStore/tables/us_accidents  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚ spark.read.csv()
                                         â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚     ETL Notebook (01_ingest_...)    â”‚
                     â”‚   - Extract CSV                      â”‚
                     â”‚   - Clean + Transform                â”‚
                     â”‚   - Feature Engineering              â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚ write.format("delta")
                                        â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚           Delta Lake Storage            â”‚
                   â”‚   dbfs:/mnt/delta/us_accidents_clean   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ spark.sql() + SQL views
                                   â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚         Exploratory Analysis Notebook (02_eda)          â”‚
          â”‚  - State trends                                          â”‚
          â”‚  - Hourly accident patterns                              â”‚
          â”‚  - Weather correlations                                  â”‚
          â”‚  - Summary tables for Power BI                           â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚ export tables as CSV
                                     â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚              Power BI Desktop             â”‚
                  â”‚   - Hotspot analysis                     â”‚
                  â”‚   - Severity dashboards                  â”‚
                  â”‚   - Trend analysis                       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                     MACHINE LEARNING PIPELINE (PARALLEL)
                     -------------------------------------

                                        â–²
                                        â”‚ spark.table()
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚            ML Notebook (03_ml)            â”‚
                   â”‚   - Convert sample to pandas              â”‚
                   â”‚   - scikit-learn Logistic Regression      â”‚
                   â”‚   - Predict severity                      â”‚
                   â”‚   - Accuracy + classification report      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚ %run automation
                                        â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚       Simulated ML Job Trigger (04_job)   â”‚
                  â”‚   Orchestrates ML pipeline steps           â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
![Architecture Diagram](Architecture_image.png)

---

## ğŸ“¦ Tools & Technologies

| Category        | Tools / Services                                      |
|----------------|--------------------------------------------------------|
| Platform       | Databricks Community Edition / Databricks Workspace   |
| Storage        | DBFS (FileStore), Delta Lake                          |
| Processing     | PySpark, SQL, Pandas                                  |
| Machine Learning | scikit-learn (Logistic Regression)                  |
| Orchestration  | Databricks notebook `%run` trigger                    |
| Visualization  | Any BI / plotting tool (e.g. matplotlib, Power BI, etc.) |
| Data Source    | US Accidents dataset (Kaggle)                          |

---

## ğŸ—‚ Dataset: US Accidents

The **US Accidents** dataset contains several million accident records collected via various traffic APIs. It includes:

- Accident ID and timestamps  
- Latitude & longitude  
- City, state  
- Severity (1â€“4)  
- Distance, duration  
- Weather and road condition fields (depending on version)

This project uses:

- **Time-based features** (`start_hour`, `duration_minutes`)  
- **Location features** (`state`)  
- **Engineered flags** (`is_weekend`)  

to support analytics and severity prediction.

---

## ğŸ”„ ELT Pipeline

### 1ï¸âƒ£ Extract â€“ From Kaggle â†’ Google Drive â†’ Databricks

Due to upload limits in Databricks CE, the CSV is stored on **Google Drive** and pulled into **DBFS** using `wget`:

```bash
%sh
wget "https://drive.google.com/uc?export=download&id=<FILE_ID>" \
  -O /dbfs/FileStore/tables/us_accidents.csv
```

### 2ï¸âƒ£ Load â€“ Persist to Delta Lake

The cleaned DataFrame is written as a **Delta table**:


```python
DELTA_PATH = "/Volumes/workspace/default/usaccidents_volume/us_accidents_delta"

final_df.write \
    .format("delta") \
    .mode("overwrite") \
    .save(DELTA_PATH)

spark.sql("""
CREATE TABLE IF NOT EXISTS default.us_accidents_clean
USING DELTA
LOCATION 'dbfs:/mnt/delta/us_accidents_clean'
""")

```
### 3ï¸âƒ£ Transform â€“ Cleaning & Feature Engineering (PySpark)
Key transformations:

* Parse timestamps

* Compute accident duration in minutes

* Extract hour of day

* Weekend / weekday flag

* Basic null handling

Example:
```python
from pyspark.sql import functions as F

df = df_raw.withColumn(
    "start_time", F.to_timestamp("Start_Time")
).withColumn(
    "end_time", F.to_timestamp("End_Time")
).withColumn(
    "duration_minutes",
    (F.col("end_time").cast("long") - F.col("start_time").cast("long")) / 60.0
).withColumn(
    "start_hour", F.hour("start_time")
).withColumn(
    "is_weekend",
    F.dayofweek("start_time").isin([1, 7]).cast("boolean")
).withColumn(
    "distance_miles", F.col("Distance(mi)").cast("double")
)

```
The final Delta table `madsc102.usaccidents_volume` is used for SQL, EDA, and ML.

### ğŸ“Š Exploratory Analysis

The `02_exploratory_analysis` notebook uses SQL + DataFrame API to explore:

* Daily accident counts

* Top accident states

* Peak accident hours

* Severity distribution
Example
```sql
-- Daily trend
SELECT start_date,
       COUNT(*) AS total_accidents,
       ROUND(AVG(duration_minutes), 2) AS avg_duration_min
FROM default.us_accidents_clean
GROUP BY start_date
ORDER BY start_date DESC
LIMIT 100;

-- Top states by accident count
SELECT state,
       COUNT(*) AS accidents
FROM default.us_accidents_clean
GROUP BY state
ORDER BY accidents DESC
LIMIT 10;
```
### ğŸ¤– Machine Learning (scikit-learn)

Spark ML models exceed size limits under Spark Connect in CE, so ML is built using pandas + scikit-learn on a manageable sample.

Features:

* start_hour

* distance_miles

* duration_minutes

* is_weekend

**Target:**

* severity (int)

Example `from 03_ml_model_optional.py`:
```python
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

df_full = spark.table("default.us_accidents_clean")
pdf = df_full.limit(5000).toPandas()

features = ["start_hour", "distance_miles", "duration_minutes", "is_weekend"]
X = pdf[features].fillna(0)
y = pdf["severity"].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

lr = LogisticRegression(max_iter=200)
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```
Predictions are converted back to a Spark DataFrame for display in Databricks.

### ğŸ” Orchestration â€“ Job Trigger Notebook

Databricks CE does not support full Workflows / Jobs.
Instead, a simple trigger notebook is provided to run the ML step:
```python
print("=== ML Job Triggered ===")

%run /Workspace/Users/<your-user>/03_ml_model_optional.py

print("=== ML Job Completed ===")
```
This simulates a job-like execution order

### ğŸ“ˆ Visualization Layer

The final Visualization layer is left tool-agnostic.
You can:

* Use Databricks SQL dashboards

* Export aggregated tables or views as CSV for external tools

* Build charts in Power BI, Tableau, or matplotlib/seaborn

**Typical visuals:**

* Accidents over time

* Accidents by state and severity

* Hour-of-day heatmaps

* Actual vs predicted severity distribution (from ML)

### ğŸ—‚ Project Structure
```
               .
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_ingest_clean_write_delta.py      # Ingestion + cleaning + Delta write
â”‚   â”œâ”€â”€ 02_exploratory_analysis.py          # EDA & SQL-based exploration
â”‚   â”œâ”€â”€ 03_ml_model_optional.py             # scikit-learn ML model
â”‚   â””â”€â”€ 04_ml_job_trigger.py                # Simple ML job trigger
â”‚
â”œâ”€â”€ architecture.png/
â”‚                                           # Architecture diagram for README
â”‚
â”œâ”€â”€ deployment/
â”‚   â””â”€â”€ databricks_job_ml.json              # Reference job config (for full Databricks)
â”‚
â””â”€â”€ README.md

```
### ğŸŒŸ Author Notes

This project was developed for the **MADSC102 â€“ Unlocking the Power of Big Data** module, showcasing:

* Practical Databricks ELT design

* Use of Delta Lake as central storage

* Balancing platform limitations (Spark Connect ML) with scikit-learn

* Combining engineering, analytics, and ML in one coherent pipeline

Feel free to fork, extend, or adapt this pipeline to other large-scale datasets.
